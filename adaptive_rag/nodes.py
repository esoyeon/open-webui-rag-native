"""
Graph Nodes for Adaptive RAG
LangGraphì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°ì¢… ë…¸ë“œ í•¨ìˆ˜ë“¤
"""

import logging
import os
from typing import List, Optional
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults

logger = logging.getLogger(__name__)


class WebSearchTool:
    """Tavily ê¸°ë°˜ ì‹¤ì œ ì›¹ ê²€ìƒ‰ ë„êµ¬"""

    def __init__(self):
        """Tavily APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ ì´ˆê¸°í™”"""
        try:
            self.tavily_tool = TavilySearchResults(
                max_results=3,
                search_depth="advanced",
                include_answer=True,
                include_raw_content=True,
                include_images=False,
                # API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ TAVILY_API_KEYì—ì„œ ìë™ìœ¼ë¡œ ë¡œë“œ
            )
            logger.info("âœ… Tavily ì›¹ ê²€ìƒ‰ ë„êµ¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ Tavily ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # Fallback to mock for development
            self.tavily_tool = None
            logger.warning("Mock ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")

    def search(self, query: str, max_results: int = 3) -> List[dict]:
        """ì‹¤ì œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        logger.info(f"ğŸ” Tavily ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")

        if self.tavily_tool is None:
            # Fallback mock results if Tavily is not available
            logger.warning("âš ï¸ Tavily ì‚¬ìš© ë¶ˆê°€, Mock ê²°ê³¼ ë°˜í™˜")
            return [
                {
                    "content": f"Fallback search result for: {query}",
                    "url": "https://example.com/fallback",
                    "title": f"Fallback Result for {query}",
                }
            ]

        try:
            # Tavily ê²€ìƒ‰ ìˆ˜í–‰
            results = self.tavily_tool.invoke({"query": query})

            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for result in results[:max_results]:
                formatted_result = {
                    "content": result.get("content", ""),
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                }
                formatted_results.append(formatted_result)

            logger.info(f"âœ… Tavilyì—ì„œ {len(formatted_results)}ê°œ ê²°ê³¼ ë°˜í™˜")
            return formatted_results

        except Exception as e:
            logger.error(f"âŒ Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # Return fallback result on error
            return [
                {
                    "content": f"ê²€ìƒ‰ ì˜¤ë¥˜ ë°œìƒ. ì§ˆë¬¸: {query}ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "url": "https://error.com",
                    "title": f"ê²€ìƒ‰ ì˜¤ë¥˜: {query}",
                }
            ]


class RAGNodes:
    """Adaptive RAGë¥¼ ìœ„í•œ ë…¸ë“œ í•¨ìˆ˜ë“¤"""

    def __init__(
        self,
        vector_store=None,
        model_name: str = "gpt-3.5-turbo",
        temperature: float = 0,
    ):
        self.vector_store = vector_store
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.web_search_tool = WebSearchTool()

        # RAG í”„ë¡¬í”„íŠ¸ ì„¤ì •
        rag_template = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ì—°êµ¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
ğŸ“‹ **êµ¬ì¡°í™”ëœ ë‹µë³€**: ì£¼ì œë³„ë¡œ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…
ğŸ” **ìƒì„¸í•œ ë¶„ì„**: í•µì‹¬ ë‚´ìš©, ë°°ê²½, ì˜í–¥, ì˜ë¯¸ ë“±ì„ í¬í•¨
ğŸ“Š **ë¹„êµ ë¶„ì„**: ì—¬ëŸ¬ êµ­ê°€/ê¸°ì—…/ì •ì±…ì´ ì–¸ê¸‰ëœ ê²½ìš° ë¹„êµí‘œë‚˜ ì°¨ì´ì  ëª…ì‹œ
ğŸ’¡ **ì‹¤ìš©ì  ì •ë³´**: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë‚ ì§œ, ì •ì±…ëª…, ê¸°ê´€ëª… ë“± í¬í•¨
ğŸ¯ **ê²°ë¡  ë° ì‹œì‚¬ì **: í•µì‹¬ ìš”ì•½ê³¼ í–¥í›„ ì „ë§ ì œì‹œ

**ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ:**
## ğŸ“‹ í•µì‹¬ ë‚´ìš©
[ì£¼ìš” ë‚´ìš© ì„¤ëª…]

## ğŸ” ìƒì„¸ ë¶„ì„  
[êµ¬ì²´ì  ë¶„ì„ ë‚´ìš©]

## ğŸ“Š ë¹„êµ/íŠ¹ì§•
[ë¹„êµ ë¶„ì„ ë˜ëŠ” ì£¼ìš” íŠ¹ì§•]

## ğŸ’¡ ì‹œì‚¬ì 
[ì˜ë¯¸ì™€ ì „ë§]

**ì‚¬ìš©ì ì§ˆë¬¸ì´ í•œêµ­ì–´ë©´ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ, ì˜ì–´ë©´ ì˜ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.**

---
**ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´:**
{context}

**ì§ˆë¬¸:** {question}

**ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ ë‹µë³€:**"""

        self.rag_prompt = ChatPromptTemplate.from_template(rag_template)
        self.rag_chain = self.rag_prompt | self.llm | StrOutputParser()

    def retrieve(self, state: dict) -> dict:
        """ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
        logger.info("==== [RETRIEVE] ====")
        question = state["question"]

        if not self.vector_store:
            logger.error("Vector store not initialized")
            return {"documents": []}

        try:
            # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
            documents = self.vector_store.similarity_search(question, k=10)
            logger.info(f"Retrieved {len(documents)} documents")
            return {"documents": documents}
        except Exception as e:
            logger.error(f"Error in document retrieval: {e}")
            return {"documents": []}

    def generate(self, state: dict) -> dict:
        """ë‹µë³€ ìƒì„± ë…¸ë“œ"""
        logger.info("==== [GENERATE] ====")
        question = state["question"]
        documents = state.get("documents", [])

        if not documents:
            # ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ë‹µë³€
            generation = (
                "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        else:
            try:
                # ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                context = self._format_docs(documents)

                # RAG ë‹µë³€ ìƒì„±
                generation = self.rag_chain.invoke(
                    {"context": context, "question": question}
                )
            except Exception as e:
                logger.error(f"Error in answer generation: {e}")
                generation = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        return {"generation": generation}

    def web_search(self, state: dict) -> dict:
        """ì›¹ ê²€ìƒ‰ ë…¸ë“œ"""
        logger.info("==== [WEB SEARCH] ====")
        question = state["question"]

        try:
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            web_results = self.web_search_tool.search(question, max_results=3)

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
            web_results_docs = [
                Document(
                    page_content=result["content"],
                    metadata={
                        "source": result["url"],
                        "title": result.get("title", ""),
                    },
                )
                for result in web_results
            ]

            logger.info(f"Found {len(web_results_docs)} web search results")
            return {"documents": web_results_docs}

        except Exception as e:
            logger.error(f"Error in web search: {e}")
            return {"documents": []}

    def decide_to_generate(self, state: dict) -> str:
        """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ í›„ ìƒì„± ì—¬ë¶€ ê²°ì •"""
        logger.info("==== [DECISION TO GENERATE] ====")
        filtered_documents = state.get("documents", [])
        retry_count = state.get("retry_count", 0)

        if not filtered_documents:
            # ì¬ì‹œë„ íšŸìˆ˜ê°€ 3ë²ˆì„ ì´ˆê³¼í•œ ê²½ìš° ê°•ì œë¡œ ë‹µë³€ ìƒì„±
            if retry_count >= 3:
                logger.info("==== [DECISION: RETRY LIMIT REACHED, FORCE GENERATE] ====")
                return "generate"
            else:
                # ëª¨ë“  ë¬¸ì„œê°€ ê´€ë ¨ì„± ì—†ëŠ” ê²½ìš° ì§ˆë¬¸ ì¬ì‘ì„±
                logger.info(
                    "==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ===="
                )
                return "transform_query"
        else:
            # ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° ë‹µë³€ ìƒì„±
            logger.info("==== [DECISION: GENERATE] ====")
            return "generate"

    def _format_docs(self, docs: List[Document]) -> str:
        """ë¬¸ì„œë“¤ì„ RAG ì²´ì¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í¬ë§·íŒ…"""
        if not docs:
            return ""

        formatted_parts = []
        for i, doc in enumerate(docs):
            source = doc.metadata.get("source", "Unknown")
            content = doc.page_content
            formatted_parts.append(f"Document {i+1} (Source: {source}):\n{content}")

        return "\n\n".join(formatted_parts)
