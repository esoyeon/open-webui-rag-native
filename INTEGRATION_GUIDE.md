# ğŸ”— Open WebUI + ìì²´ RAG/DB/Embedding API ì™„ì „ í†µí•© ê°€ì´ë“œ

> **Open WebUIë¥¼ ìì²´ ë°ì´í„°ë² ì´ìŠ¤, RAG ì‹œìŠ¤í…œ, Embedding APIì™€ í†µí•©í•˜ëŠ” ë‹¨ê³„ë³„ ì‹¤ì „ ê°€ì´ë“œ**

## ğŸ¯ **ê°€ì´ë“œ ëª©ì **

ì´ ê°€ì´ë“œëŠ” **Open WebUI**ë¥¼ ê¸°ì¡´ì˜ OpenAI API ëŒ€ì‹  **ìì²´ì ì¸ RAG ì‹œìŠ¤í…œ, ë°ì´í„°ë² ì´ìŠ¤, Embedding API**ì™€ ì—°ê²°í•˜ë ¤ëŠ” ëª¨ë“  ê°œë°œìë¥¼ ìœ„í•œ ì™„ì „í•œ ë§¤ë‰´ì–¼ì…ë‹ˆë‹¤.

### **ì ìš© ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤**
- ğŸ¢ **ê¸°ì—… ë‚´ë¶€ ë¬¸ì„œ** ê¸°ë°˜ AI ì±—ë´‡
- ğŸ“š **ì „ë¬¸ ë„ë©”ì¸ ì§€ì‹** ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
- ğŸ”’ **í”„ë¼ì´ë¹— ë°ì´í„°** ë³´ì•ˆì´ ì¤‘ìš”í•œ í™˜ê²½
- ğŸ’° **ë¹„ìš© ìµœì í™”**ë¥¼ ìœ„í•œ ìì²´ ì¸í”„ë¼ í™œìš©
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›**ì´ í•„ìš”í•œ íŠ¹í™” ì‹œìŠ¤í…œ

---

## ğŸ“‹ **í†µí•© ë°©ë²• ê°œìš”**

Open WebUIì™€ ìì²´ ì‹œìŠ¤í…œì„ í†µí•©í•˜ëŠ” **3ê°€ì§€ ì£¼ìš” ë°©ë²•**:

### **1. OpenAI í˜¸í™˜ API ì„œë²„ ë°©ì‹** â­ (ê¶Œì¥)
- âœ… **ê°€ì¥ ì•ˆì •ì ì´ê³  ë²”ìš©ì **
- âœ… ê¸°ì¡´ OpenAI í´ë¼ì´ì–¸íŠ¸ì™€ ì™„ì „ í˜¸í™˜
- âœ… Docker í™˜ê²½ì—ì„œ ì•ˆì •ì  ì‘ë™
- âœ… ë‹¤ë¥¸ ë„êµ¬ë“¤ê³¼ë„ ì—°ë™ ê°€ëŠ¥

### **2. Pipelines Plugin Framework ë°©ì‹**
- âœ… Open WebUI ë„¤ì´í‹°ë¸Œ í†µí•©
- âœ… ë” ê¹Šì€ ë ˆë²¨ì˜ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
- âš ï¸ Open WebUI ì „ìš©, ìƒëŒ€ì ìœ¼ë¡œ ë³µì¡

### **3. ì§ì ‘ FastAPI í†µí•© ë°©ì‹**
- âœ… ì™„ì „í•œ ì œì–´ ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•
- âš ï¸ ë†’ì€ ê°œë°œ ë³µì¡ë„
- âš ï¸ ìœ ì§€ë³´ìˆ˜ ë¶€ë‹´

---

## ğŸš€ **ë°©ë²• 1: OpenAI í˜¸í™˜ API ì„œë²„** (ê¶Œì¥)

### **í•µì‹¬ ì•„ì´ë””ì–´**
OpenAI APIì™€ ë™ì¼í•œ ì—”ë“œí¬ì¸íŠ¸(`/v1/chat/completions`, `/v1/models`)ë¥¼ ì œê³µí•˜ëŠ” FastAPI ì„œë²„ë¥¼ ë§Œë“¤ì–´, Open WebUIê°€ ë§ˆì¹˜ OpenAIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ìì²´ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ê²Œ í•©ë‹ˆë‹¤.

### **1ë‹¨ê³„: ê¸°ë³¸ FastAPI ì„œë²„ êµ¬ì¡°**

```python
# web_api_server.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any
import uvicorn

app = FastAPI(title="Custom RAG API Server")

# CORS ì„¤ì • (Open WebUI ì—°ë™ì„ ìœ„í•´ í•„ìˆ˜)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI í˜¸í™˜ ë°ì´í„° ëª¨ë¸
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: int = 2000

class ChatCompletionResponse(BaseModel):
    id: str = "chatcmpl-custom"
    object: str = "chat.completion"
    created: int = 1677610602
    model: str
    choices: List[Dict[str, Any]]

# í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸ 1: ëª¨ë¸ ëª©ë¡
@app.get("/v1/models")
async def get_models():
    return {
        "data": [
            {
                "id": "your-custom-model",
                "object": "model", 
                "created": 1677610602,
                "owned_by": "custom-api",
            }
        ]
    }

# í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸ 2: ì±„íŒ… ì™„ë£Œ
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    try:
        # ì—¬ê¸°ì— ìì²´ RAG ë¡œì§ ì—°ê²°
        answer = process_with_your_rag_system(request.messages[-1].content)
        
        return ChatCompletionResponse(
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant", 
                    "content": answer
                },
                "finish_reason": "stop"
            }]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def process_with_your_rag_system(user_question: str) -> str:
    """ì—¬ê¸°ì— ìì²´ RAG ì‹œìŠ¤í…œ ë¡œì§ì„ êµ¬í˜„"""
    # ì˜ˆì‹œ: ë²¡í„° ê²€ìƒ‰ + LLM ìƒì„±
    documents = your_vector_store.search(user_question)
    context = "\n".join([doc.content for doc in documents])
    
    prompt = f"Context: {context}\n\nQuestion: {user_question}\n\nAnswer:"
    answer = your_llm.generate(prompt)
    
    return answer

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### **2ë‹¨ê³„: RAG ì‹œìŠ¤í…œ í†µí•©**

#### **A. ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°**
```python
# vector_store.py
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class CustomVectorStore:
    def __init__(self, index_path: str, documents_path: str):
        self.index = faiss.read_index(index_path)
        self.documents = self.load_documents(documents_path)
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def search(self, query: str, k: int = 5):
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_vector = self.encoder.encode([query])
        
        # FAISS ê²€ìƒ‰
        scores, indices = self.index.search(query_vector, k)
        
        # ë¬¸ì„œ ë°˜í™˜
        return [self.documents[idx] for idx in indices[0]]
```

#### **B. LLM ìƒì„±ê¸° ì—°ê²°**
```python
# llm_generator.py
from openai import OpenAI
# ë˜ëŠ” from transformers import pipeline

class CustomLLMGenerator:
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.client = OpenAI()  # ë˜ëŠ” ë¡œì»¬ ëª¨ë¸
        self.model_name = model_name
    
    def generate(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content
```

### **3ë‹¨ê³„: Open WebUI ì„¤ì •**

#### **Dockerë¡œ Open WebUI ì‹¤í–‰**
```bash
docker run -d \
  --name open-webui \
  -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8000/v1 \
  -e OPENAI_API_KEY=dummy-key \
  -v open-webui:/app/backend/data \
  ghcr.io/open-webui/open-webui:main
```

#### **ë¸Œë¼ìš°ì €ì—ì„œ ìˆ˜ë™ ì„¤ì •**
1. **http://localhost:3000** ì ‘ì†
2. **Admin Panel** â†’ **Settings** â†’ **Connections**
3. **OpenAI API** ì„¹ì…˜ì—ì„œ:
   - **API Base URL**: `http://host.docker.internal:8000/v1`
   - **API Key**: `dummy-key` (ì•„ë¬´ ê°’)

### **4ë‹¨ê³„: ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€**

#### **A. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›**
```python
from fastapi.responses import StreamingResponse
import json

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    if request.stream:
        return StreamingResponse(
            stream_response(request), 
            media_type="text/plain"
        )
    # ... ê¸°ì¡´ ë¡œì§

async def stream_response(request):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    answer = process_with_your_rag_system(request.messages[-1].content)
    
    # í† í°ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
    for token in answer.split():
        chunk = {
            "id": "chatcmpl-custom",
            "object": "chat.completion.chunk",
            "choices": [{
                "index": 0,
                "delta": {"content": token + " "},
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(chunk)}\n\n"
    
    # ì¢…ë£Œ ì‹ í˜¸
    yield f"data: [DONE]\n\n"
```

#### **B. ë¬¸ì„œ ì—…ë¡œë“œ ì§€ì›**
```python
from fastapi import UploadFile, File

@app.post("/api/documents")
async def upload_document(file: UploadFile = File(...)):
    """PDF ë¬¸ì„œ ì—…ë¡œë“œ ë° ë²¡í„°í™”"""
    content = await file.read()
    
    # PDF ì²˜ë¦¬ ë° ì²­í‚¹
    documents = process_pdf(content)
    
    # ë²¡í„°í™” ë° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸  
    embeddings = embed_documents(documents)
    update_vector_index(embeddings, documents)
    
    return {"message": f"{file.filename} ì—…ë¡œë“œ ì™„ë£Œ"}
```

---

## ğŸ”§ **ë°©ë²• 2: Pipelines Plugin Framework**

### **í•µì‹¬ ì•„ì´ë””ì–´**
Open WebUIì˜ Pipelines Plugin ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ë” ê¹Šì€ ë ˆë²¨ì—ì„œ í†µí•©í•©ë‹ˆë‹¤.

### **1ë‹¨ê³„: Pipe í´ë˜ìŠ¤ êµ¬í˜„**

```python
# pipelines/custom_rag_pipeline.py
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

class Pipeline:
    class Valves(BaseModel):
        priority: int = 0
        temperature: float = 0.7
        
    def __init__(self):
        self.type = "manifold"
        self.name = "Custom RAG Pipeline"
        self.valves = self.Valves()
        
        # ìì²´ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.vector_store = load_vector_store()
        self.llm = load_llm_model()
    
    async def on_startup(self):
        """íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
        print("Custom RAG Pipeline ì‹œì‘")
    
    async def on_shutdown(self):
        """íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
        print("Custom RAG Pipeline ì¢…ë£Œ")
    
    def pipe(
        self, 
        user_message: str, 
        model_id: str, 
        messages: List[Dict[str, str]], 
        body: Dict[str, Any]
    ) -> str:
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        try:
            # ë²¡í„° ê²€ìƒ‰
            documents = self.vector_store.search(user_message)
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n".join([doc.content for doc in documents])
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"Context: {context}\n\nQuestion: {user_message}\n\nAnswer:"
            
            # LLM ìƒì„±
            answer = self.llm.generate(prompt)
            
            return answer
            
        except Exception as e:
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
```

### **2ë‹¨ê³„: Pipelines ì„œë²„ ì‹¤í–‰**

```python
# pipelines_server.py
from fastapi import FastAPI
from pipelines.custom_rag_pipeline import Pipeline

app = FastAPI()

# íŒŒì´í”„ë¼ì¸ ë“±ë¡
pipeline = Pipeline()

@app.get("/")
async def get_status():
    return {"status": "Pipelines Server Running"}

@app.post("/v1/chat/completions")
async def chat_completions(request: dict):
    messages = request.get("messages", [])
    user_message = messages[-1]["content"] if messages else ""
    
    response = pipeline.pipe(
        user_message=user_message,
        model_id=request.get("model", ""),
        messages=messages,
        body=request
    )
    
    return {
        "choices": [{
            "message": {
                "role": "assistant",
                "content": response
            }
        }]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=9099)
```

### **3ë‹¨ê³„: Open WebUIì—ì„œ Pipelines ì„¤ì •**
1. **Admin Panel** â†’ **Settings** â†’ **Pipelines**
2. **Pipeline URL**: `http://host.docker.internal:9099`
3. íŒŒì´í”„ë¼ì¸ ëª©ë¡ì—ì„œ **Custom RAG Pipeline** ì„ íƒ

---

## ğŸ—ï¸ **ë°©ë²• 3: ì™„ì „ ì»¤ìŠ¤í…€ í†µí•©**

### **ê³ ê¸‰ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì™„ì „í•œ ì œì–´**

```python
# custom_integration.py
from fastapi import FastAPI, WebSocket
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse

app = FastAPI()

# ì •ì  íŒŒì¼ ì„œë¹™ (í”„ë¡ íŠ¸ì—”ë“œ)
app.mount("/static", StaticFiles(directory="frontend/dist"), name="static")

@app.get("/")
async def get_frontend():
    """ì»¤ìŠ¤í…€ í”„ë¡ íŠ¸ì—”ë“œ ì œê³µ"""
    return HTMLResponse(open("frontend/dist/index.html").read())

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """ì‹¤ì‹œê°„ ì±„íŒ… WebSocket"""
    await websocket.accept()
    
    while True:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜ì‹ 
        user_message = await websocket.receive_text()
        
        # ìì²´ RAG ì²˜ë¦¬
        response = process_message(user_message)
        
        # ì‘ë‹µ ì „ì†¡
        await websocket.send_text(response)

@app.post("/api/chat")
async def chat_api(request: dict):
    """REST API ì—”ë“œí¬ì¸íŠ¸"""
    message = request.get("message")
    return {"response": process_message(message)}

def process_message(message: str) -> str:
    """ìì²´ RAG ë¡œì§"""
    # ë²¡í„° ê²€ìƒ‰ + LLM ìƒì„±
    return "AI ì‘ë‹µ"
```

---

## ğŸ” **í•µì‹¬ êµ¬í˜„ íŒ¨í„´**

### **1. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**
```python
def create_document_pipeline():
    """ë¬¸ì„œ â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì¸ë±ì‹±"""
    
    # PDF ë¡œë”
    loader = PyPDFLoader()
    
    # í…ìŠ¤íŠ¸ ë¶„í• ê¸°  
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    
    # ì„ë² ë”© ëª¨ë¸
    embeddings = OpenAIEmbeddings()
    # ë˜ëŠ” ë¡œì»¬: HuggingFaceEmbeddings()
    
    # ë²¡í„° ìŠ¤í† ì–´
    vector_store = FAISS.from_documents(
        documents=documents,
        embedding=embeddings
    )
    
    return vector_store
```

### **2. ê²€ìƒ‰ ìµœì í™”**
```python
def hybrid_search(query: str, vector_store, bm25_retriever):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + í‚¤ì›Œë“œ"""
    
    # ë²¡í„° ê²€ìƒ‰
    vector_docs = vector_store.similarity_search(query, k=5)
    
    # BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
    bm25_docs = bm25_retriever.get_relevant_documents(query)
    
    # ê²°ê³¼ ê²°í•© ë° ì¬ë­í‚¹
    combined_docs = combine_and_rerank(vector_docs, bm25_docs)
    
    return combined_docs
```

### **3. ì‘ë‹µ í’ˆì§ˆ ê°œì„ **
```python
def generate_enhanced_response(query: str, documents: List[Document]):
    """êµ¬ì¡°í™”ëœ ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„±"""
    
    context = "\n".join([doc.page_content for doc in documents])
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    **ë‹µë³€ êµ¬ì¡°:**
    ## ğŸ“‹ í•µì‹¬ ë‚´ìš©
    [ì£¼ìš” ë‚´ìš©]
    
    ## ğŸ” ìƒì„¸ ë¶„ì„
    [êµ¬ì²´ì  ë¶„ì„]
    
    ## ğŸ“Š ê´€ë ¨ ë°ì´í„°
    [ìˆ˜ì¹˜, í†µê³„ ë“±]
    
    ## ğŸ’¡ ê²°ë¡ 
    [ìš”ì•½ ë° ì‹œì‚¬ì ]
    
    ë¬¸ì„œ: {context}
    
    ì§ˆë¬¸: {query}
    
    ë‹µë³€:
    """
    
    return llm.invoke(prompt)
```

---

## ğŸ“Š **ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ**

### **1. ë²¡í„° ìŠ¤í† ì–´ ìµœì í™”**
```python
# FAISS ì¸ë±ìŠ¤ ìµœì í™”
index = faiss.IndexFlatIP(dimension)  # ë‚´ì  ê¸°ë°˜ (ë” ë¹ ë¦„)
# index = faiss.IndexHNSWFlat(dimension, 32)  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

# ì¸ë±ìŠ¤ í›ˆë ¨ ë° ì••ì¶•
if len(documents) > 10000:
    index.train(embeddings)
    index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
```

### **2. ìºì‹± ì „ëµ**
```python
from functools import lru_cache
import redis

# ë©”ëª¨ë¦¬ ìºì‹œ
@lru_cache(maxsize=1000)
def cached_search(query: str):
    return vector_store.search(query)

# Redis ìºì‹œ  
redis_client = redis.Redis()

def search_with_cache(query: str):
    cache_key = f"search:{hash(query)}"
    
    # ìºì‹œ í™•ì¸
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # ê²€ìƒ‰ ìˆ˜í–‰
    results = vector_store.search(query)
    
    # ìºì‹œ ì €ì¥ (1ì‹œê°„)
    redis_client.setex(cache_key, 3600, json.dumps(results))
    
    return results
```

### **3. ë¹„ë™ê¸° ì²˜ë¦¬**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def async_rag_processing(query: str):
    """ë¹„ë™ê¸° RAG ì²˜ë¦¬"""
    
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(max_workers=4)
    
    # ë³‘ë ¬ ì²˜ë¦¬
    tasks = [
        loop.run_in_executor(executor, vector_search, query),
        loop.run_in_executor(executor, keyword_search, query),
        loop.run_in_executor(executor, web_search, query)
    ]
    
    vector_docs, keyword_docs, web_docs = await asyncio.gather(*tasks)
    
    # ê²°ê³¼ ê²°í•©
    all_docs = combine_results(vector_docs, keyword_docs, web_docs)
    
    # ë‹µë³€ ìƒì„±
    response = await generate_response(query, all_docs)
    
    return response
```

---

## ğŸ›¡ï¸ **ë³´ì•ˆ ë° ì¸ì¦**

### **API í‚¤ ì¸ì¦**
```python
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer

security = HTTPBearer()

def verify_api_key(token: str = Depends(security)):
    if token.credentials != "your-secret-api-key":
        raise HTTPException(status_code=401, detail="Invalid API key")
    return token

@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    token: str = Depends(verify_api_key)
):
    # ... ì²˜ë¦¬ ë¡œì§
```

### **ì‚¬ìš©ëŸ‰ ì œí•œ**
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/v1/chat/completions")
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ
async def chat_completions(request: Request, ...):
    # ... ì²˜ë¦¬ ë¡œì§
```

---

## ğŸš€ **ë°°í¬ ê°€ì´ë“œ**

### **Docker Composeë¡œ ì „ì²´ ì‹œìŠ¤í…œ ë°°í¬**
```yaml
# docker-compose.yml
version: '3.8'

services:
  # ìì²´ RAG API ì„œë²„
  rag-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    networks:
      - ai-network

  # Open WebUI
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    environment:
      - OPENAI_API_BASE_URL=http://rag-api:8000/v1
      - OPENAI_API_KEY=dummy-key
    depends_on:
      - rag-api
    networks:
      - ai-network
    volumes:
      - open-webui-data:/app/backend/data

  # Redis (ìºì‹±)
  redis:
    image: redis:alpine
    networks:
      - ai-network

networks:
  ai-network:
    driver: bridge

volumes:
  open-webui-data:
```

### **Kubernetes ë°°í¬**
```yaml
# k8s-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rag-api
  template:
    metadata:
      labels:
        app: rag-api
    spec:
      containers:
      - name: rag-api
        image: your-registry/rag-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-key
---
apiVersion: v1
kind: Service
metadata:
  name: rag-api-service
spec:
  selector:
    app: rag-api
  ports:
  - port: 8000
    targetPort: 8000
  type: LoadBalancer
```

---

## ğŸ“ˆ **ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…**

### **ìƒì„¸ ë¡œê¹…**
```python
import logging
from datetime import datetime

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_system.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    start_time = datetime.now()
    
    logger.info(f"ì§ˆë¬¸ ìˆ˜ì‹ : {request.messages[-1].content[:100]}...")
    
    try:
        response = process_rag(request)
        
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"ì‘ë‹µ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")
        
        return response
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise
```

### **ë©”íŠ¸ë¦­ ìˆ˜ì§‘**
```python
from prometheus_client import Counter, Histogram, generate_latest

# ë©”íŠ¸ë¦­ ì •ì˜
REQUEST_COUNT = Counter('rag_requests_total', 'ì´ ìš”ì²­ ìˆ˜')
REQUEST_DURATION = Histogram('rag_request_duration_seconds', 'ìš”ì²­ ì²˜ë¦¬ ì‹œê°„')
ERROR_COUNT = Counter('rag_errors_total', 'ì˜¤ë¥˜ ìˆ˜')

@app.get("/metrics")
async def get_metrics():
    """Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸"""
    return Response(generate_latest(), media_type="text/plain")

@REQUEST_DURATION.time()
async def timed_rag_processing(query: str):
    REQUEST_COUNT.inc()
    
    try:
        result = process_rag(query)
        return result
    except Exception as e:
        ERROR_COUNT.inc()
        raise
```

---

## ğŸ¯ **ê²°ë¡ **

ì´ ê°€ì´ë“œë¥¼ í†µí•´ **Open WebUIë¥¼ ìì²´ RAG/DB/Embedding APIì™€ ì™„ë²½í•˜ê²Œ í†µí•©**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### **ê¶Œì¥ ì ‘ê·¼ë²•**
1. **ì‹œì‘**: OpenAI í˜¸í™˜ API ì„œë²„ë¡œ í”„ë¡œí† íƒ€ì… êµ¬ì¶•
2. **ë°œì „**: ì„±ëŠ¥ ìµœì í™” ë° ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€  
3. **ìš´ì˜**: Docker/K8së¡œ í”„ë¡œë•ì…˜ ë°°í¬

### **ì„±ê³µì„ ìœ„í•œ í•µì‹¬ í¬ì¸íŠ¸**
- âœ… **í‘œì¤€ ì¤€ìˆ˜**: OpenAI API ìŠ¤í™ ì™„ë²½ êµ¬í˜„
- âœ… **ì„±ëŠ¥ ìµœì í™”**: ìºì‹±, ë¹„ë™ê¸° ì²˜ë¦¬, ì¸ë±ìŠ¤ íŠœë‹
- âœ… **ì•ˆì •ì„±**: ì—ëŸ¬ ì²˜ë¦¬, ë¡œê¹…, ëª¨ë‹ˆí„°ë§
- âœ… **í™•ì¥ì„±**: ì»¨í…Œì´ë„ˆí™”, ë¡œë“œë°¸ëŸ°ì‹±

**ğŸš€ ì´ì œ ìì‹ ë§Œì˜ AI í†µí•© ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ì„¸ìš”!**
