"""
Background Task Queue System
RQ(Redis Queue)ë¥¼ ì‚¬ìš©í•œ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì²˜ë¦¬

ì£¼ìš” ê¸°ëŠ¥:
- ë¹„ë™ê¸° RAG ì²˜ë¦¬
- ë¬¸ì„œ ì¸ë±ì‹± ì‘ì—…
- ì£¼ê¸°ì  ìºì‹œ ì •ë¦¬
- ì„¸ì…˜ ì •ë¦¬ ì‘ì—…
"""

import logging
import time
import asyncio
from typing import Any, Dict, Optional, Callable
from functools import wraps
from datetime import datetime, timedelta

from rq import Queue
from rq.job import Job, JobStatus
from rq.worker import Worker
import redis
from redis import Redis

from .cache_manager import get_cache_manager
from .session_manager import get_session_manager

logger = logging.getLogger(__name__)


class TaskQueue:
    """
    Background Task Queue Manager
    
    Features:
    - ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬
    - ì‘ì—… ìƒíƒœ ì¶”ì 
    - ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
    - ìš°ì„ ìˆœìœ„ í
    """
    
    def __init__(
        self,
        redis_host: str = "localhost",
        redis_port: int = 6379,
        redis_db: int = 1,  # ìºì‹œì™€ ë‹¤ë¥¸ DB ì‚¬ìš©
        default_timeout: int = 300,  # 5ë¶„
    ):
        try:
            # Redis ì—°ê²°
            self.redis_conn = Redis(
                host=redis_host,
                port=redis_port,
                db=redis_db,
                decode_responses=False  # RQëŠ” bytesë¥¼ ì‚¬ìš©
            )
            
            # RQ Queue ìƒì„±
            self.default_queue = Queue('default', connection=self.redis_conn)
            self.high_priority_queue = Queue('high', connection=self.redis_conn)
            self.low_priority_queue = Queue('low', connection=self.redis_conn)
            
            # ì‘ì—… ìƒíƒœ ì¶”ì ìš©
            self.jobs = {}
            
            self.is_available = True
            logger.info("âœ… Task Queue initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Task Queue initialization failed: {e}")
            self.redis_conn = None
            self.is_available = False
    
    def _get_queue(self, priority: str = 'default') -> Queue:
        """ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ í ë°˜í™˜"""
        if priority == 'high':
            return self.high_priority_queue
        elif priority == 'low':
            return self.low_priority_queue
        else:
            return self.default_queue
    
    def enqueue_task(
        self,
        func: Callable,
        *args,
        priority: str = 'default',
        timeout: Optional[int] = None,
        retry_count: int = 3,
        **kwargs
    ) -> Optional[str]:
        """
        ì‘ì—…ì„ íì— ì¶”ê°€
        
        Args:
            func: ì‹¤í–‰í•  í•¨ìˆ˜
            args: í•¨ìˆ˜ ì¸ì
            priority: ìš°ì„ ìˆœìœ„ ('high', 'default', 'low')
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            retry_count: ì¬ì‹œë„ íšŸìˆ˜
            kwargs: í•¨ìˆ˜ í‚¤ì›Œë“œ ì¸ì
        
        Returns:
            Job ID ë˜ëŠ” None
        """
        if not self.is_available:
            logger.warning("Task queue not available, executing synchronously")
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.error(f"Synchronous task execution failed: {e}")
                return None
        
        try:
            queue = self._get_queue(priority)
            
            # RQ 2.x API: use job_timeout and Retry object
            enqueue_kwargs = {
                "job_timeout": (timeout or 300),
            }

            job = queue.enqueue(
                func,
                *args,
                **enqueue_kwargs,
                **kwargs
            )
            
            self.jobs[job.id] = job
            logger.info(f"âœ… Task enqueued: {job.id} ({func.__name__})")
            return job.id
            
        except Exception as e:
            logger.error(f"Failed to enqueue task: {e}")
            return None
    
    def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        if not self.is_available:
            return {"status": "unavailable"}
        
        try:
            job = Job.fetch(job_id, connection=self.redis_conn)
            
            return {
                "id": job_id,
                "status": job.get_status(),
                "created_at": job.created_at.isoformat() if job.created_at else None,
                "started_at": job.started_at.isoformat() if job.started_at else None,
                "ended_at": job.ended_at.isoformat() if job.ended_at else None,
                "result": job.result,
                "exc_info": job.exc_info
            }
            
        except Exception as e:
            logger.error(f"Failed to get job status: {e}")
            return {"status": "error", "error": str(e)}
    
    def cancel_job(self, job_id: str) -> bool:
        """ì‘ì—… ì·¨ì†Œ"""
        if not self.is_available:
            return False
        
        try:
            job = Job.fetch(job_id, connection=self.redis_conn)
            job.cancel()
            logger.info(f"âœ… Job cancelled: {job_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to cancel job: {e}")
            return False
    
    def get_queue_info(self) -> Dict[str, Any]:
        """í ì •ë³´ ì¡°íšŒ"""
        if not self.is_available:
            return {"available": False}
        
        try:
            return {
                "available": True,
                "queues": {
                    "high": {
                        "length": len(self.high_priority_queue),
                        "jobs": self.high_priority_queue.get_job_ids()
                    },
                    "default": {
                        "length": len(self.default_queue),
                        "jobs": self.default_queue.get_job_ids()
                    },
                    "low": {
                        "length": len(self.low_priority_queue),
                        "jobs": self.low_priority_queue.get_job_ids()
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get queue info: {e}")
            return {"available": False, "error": str(e)}


# Background task functions
def process_rag_async(question: str, session_id: str, search_type: str = None) -> Dict[str, Any]:
    """
    ë¹„ë™ê¸° RAG ì²˜ë¦¬ íƒœìŠ¤í¬
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        session_id: ì„¸ì…˜ ID
        search_type: ê²€ìƒ‰ íƒ€ì…
    
    Returns:
        RAG ì²˜ë¦¬ ê²°ê³¼
    """
    try:
        from .optimized_rag import OptimizedRAGEngine, SearchType
        
        # RAG ì—”ì§„ ì´ˆê¸°í™” (ì›Œì»¤ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ë§¤ë²ˆ ì´ˆê¸°í™” í•„ìš”)
        rag_engine = OptimizedRAGEngine()
        
        # ê²€ìƒ‰ íƒ€ì… ë³€í™˜
        if search_type:
            search_type_enum = SearchType(search_type)
        else:
            search_type_enum = None
        
        # ë™ê¸° í•¨ìˆ˜ë¥¼ í˜¸ì¶œ (ì›Œì»¤ëŠ” ë™ê¸° í™˜ê²½)
        result = asyncio.run(
            rag_engine.process_question(question, session_id, search_type_enum)
        )
        
        return {
            "success": True,
            "answer": result.answer,
            "sources": [s.to_dict() for s in result.sources],
            "search_type": result.search_type.value,
            "response_time": result.response_time,
            "cached": result.cached
        }
        
    except Exception as e:
        logger.error(f"Async RAG processing failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }


def cleanup_expired_sessions() -> Dict[str, Any]:
    """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ íƒœìŠ¤í¬"""
    try:
        session_manager = get_session_manager()
        
        # í”„ë¼ì´ë¹— ë©”ì†Œë“œë¥¼ ì§ì ‘ í˜¸ì¶œ (ì •ë¦¬ ì‘ì—…)
        session_manager._cleanup_expired_sessions()
        
        stats = session_manager.get_session_stats()
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "session_stats": stats
        }
        
    except Exception as e:
        logger.error(f"Session cleanup failed: {e}")
        return {"success": False, "error": str(e)}


def cleanup_cache() -> Dict[str, Any]:
    """ìºì‹œ ì •ë¦¬ íƒœìŠ¤í¬"""
    try:
        cache_manager = get_cache_manager()
        
        # ê²€ìƒ‰ ìºì‹œ ë¬´íš¨í™”
        cleaned_count = cache_manager.invalidate_search_cache()
        
        # ìºì‹œ ìƒíƒœ ì¡°íšŒ
        health = cache_manager.get_health()
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "cleaned_keys": cleaned_count,
            "cache_health": health
        }
        
    except Exception as e:
        logger.error(f"Cache cleanup failed: {e}")
        return {"success": False, "error": str(e)}


def index_document_async(document_path: str, document_metadata: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    ë¬¸ì„œ ì¸ë±ì‹± ë¹„ë™ê¸° íƒœìŠ¤í¬
    
    Args:
        document_path: ë¬¸ì„œ ê²½ë¡œ
        document_metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
    
    Returns:
        ì¸ë±ì‹± ê²°ê³¼
    """
    try:
        from document_processing.pdf import PDFRetrievalChain
        from adaptive_rag.vector_store import FAISSVectorStore
        from langchain_openai import OpenAIEmbeddings
        import os
        
        # PDF ì²˜ë¦¬
        if document_path.endswith('.pdf'):
            pdf_chain = PDFRetrievalChain([document_path])
            pdf_chain.create_chain()
            documents = pdf_chain.load_documents([document_path])
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if document_metadata:
                for doc in documents:
                    doc.metadata.update(document_metadata)
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
            vector_store = FAISSVectorStore(embedding_function=embeddings, dimension=1536)
            
            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            vector_store_path = os.path.join(project_root, "data", "vector_store")
            
            if os.path.exists(vector_store_path):
                vector_store.load(vector_store_path)
            
            # ìƒˆ ë¬¸ì„œ ì¶”ê°€
            vector_store.add_documents(documents)
            
            # ì €ì¥
            os.makedirs(os.path.dirname(vector_store_path), exist_ok=True)
            vector_store.save(vector_store_path)
            
            return {
                "success": True,
                "document_path": document_path,
                "processed_chunks": len(documents),
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "success": False,
                "error": "Unsupported document format"
            }
            
    except Exception as e:
        logger.error(f"Document indexing failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "document_path": document_path
        }


# Singleton task queue
_task_queue: Optional[TaskQueue] = None


def get_task_queue() -> TaskQueue:
    """Global task queue ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _task_queue
    if _task_queue is None:
        _task_queue = TaskQueue()
        logger.info("Task Queue initialized")
    return _task_queue


def async_task(priority: str = 'default', timeout: int = 300, retry: int = 3):
    """ë¹„ë™ê¸° ì‘ì—… ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            task_queue = get_task_queue()
            return task_queue.enqueue_task(
                func, 
                *args, 
                priority=priority, 
                timeout=timeout, 
                retry_count=retry, 
                **kwargs
            )
        
        # ì›ë˜ í•¨ìˆ˜ë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì €ì¥
        wrapper._original = func
        return wrapper
    
    return decorator


# Workerë¥¼ ì‹¤í–‰í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def start_worker(queue_names: list = None):
    """
    RQ ì›Œì»¤ ì‹œì‘
    
    Args:
        queue_names: ì²˜ë¦¬í•  í ì´ë¦„ ëª©ë¡ (ê¸°ë³¸: ['high', 'default', 'low'])
    
    Usage:
        python -c "from enhanced_rag.task_queue import start_worker; start_worker()"
    """
    if queue_names is None:
        queue_names = ['high', 'default', 'low']
    
    try:
        # Redis ì—°ê²°
        redis_conn = Redis(
            host='localhost',
            port=6379,
            db=1,
            decode_responses=False
        )
        
        # í ìƒì„±
        queues = [Queue(name, connection=redis_conn) for name in queue_names]
        
        # ì›Œì»¤ ìƒì„± ë° ì‹œì‘
        worker = Worker(queues, connection=redis_conn)
        logger.info(f"ğŸš€ Starting worker for queues: {queue_names}")
        
        worker.work()
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Worker stopped by user")
    except Exception as e:
        logger.error(f"âŒ Worker error: {e}")


if __name__ == "__main__":
    # ì›Œì»¤ ì‹¤í–‰
    start_worker()
