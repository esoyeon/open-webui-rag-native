"""
Optimized RAG Engine
í˜„ì—… íŒ¨í„´ì„ ì ìš©í•œ ê³ ì„±ëŠ¥ RAG ì‹œìŠ¤í…œ

Key Improvements:
- ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ê³„ì‚° ì œê±°
- ë³‘ë ¬ ê²€ìƒ‰ ì²˜ë¦¬
- ê°„ì†Œí™”ëœ íŒŒì´í”„ë¼ì¸
- Circuit breaker íŒ¨í„´
- í† í° ìµœì í™”
"""

import asyncio
import re
import hashlib
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
import json

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from .cache_manager import RAGCacheManager, get_cache_manager
from .session_manager import SessionManager, MessageRole, get_session_manager

logger = logging.getLogger(__name__)


class SearchType(str, Enum):
    VECTOR = "vector"
    WEB = "web"
    HYBRID = "hybrid"


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""
    content: str
    source: str
    score: float
    title: str = ""
    metadata: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'content': self.content,
            'source': self.source, 
            'score': self.score,
            'title': self.title,
            'metadata': self.metadata or {}
        }
    
    @classmethod
    def from_document(cls, doc: Document) -> 'SearchResult':
        """Langchain Documentì—ì„œ SearchResult ìƒì„±"""
        metadata = doc.metadata or {}
        return cls(
            content=doc.page_content,
            source=metadata.get('source', 'unknown'),
            score=metadata.get('score', 0.0),
            title=metadata.get('title', ''),
            metadata=metadata
        )


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ëª¨ë¸"""
    answer: str
    sources: List[SearchResult]
    search_type: SearchType
    response_time: float
    cached: bool = False
    tokens_used: int = 0


class OptimizedRAGEngine:
    """
    ìµœì í™”ëœ RAG ì—”ì§„
    
    Features:
    - Multi-level caching (embedding, search, answer)
    - Parallel search execution
    - Smart routing (simple keyword-based)
    - Circuit breaker for external services
    - Token optimization
    """
    
    def __init__(
        self,
        vector_store=None,
        model_name: str = "gpt-3.5-turbo",
        temperature: float = 0,
        max_search_results: int = 8,
        max_context_tokens: int = 3000
    ):
        self.vector_store = vector_store
        self.model_name = model_name
        self.temperature = temperature
        self.max_search_results = max_search_results
        self.max_context_tokens = max_context_tokens
        
        # Core components
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        self.cache = get_cache_manager()
        self.session_manager = get_session_manager()
        
        # Thread pool for parallel operations
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # Web search (optional)
        self.web_search_tool = None
        try:
            from langchain_community.tools.tavily_search import TavilySearchResults
            self.web_search_tool = TavilySearchResults(
                max_results=3,
                search_depth="advanced",
                include_answer=True,
                include_raw_content=True,
                include_images=False
            )
            logger.info("âœ… Web search tool initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ Web search not available: {e}")
        
        # Optimized prompt
        self.rag_prompt = self._create_optimized_prompt()
        self.rag_chain = self.rag_prompt | self.llm | StrOutputParser()
        
        logger.info(f"OptimizedRAGEngine initialized with model: {model_name}")
    
    def _create_optimized_prompt(self) -> ChatPromptTemplate:
        """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """ë‹¹ì‹ ì€ ì „ë¬¸ AI ì—°êµ¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
1. ğŸ“‹ **í•µì‹¬ ë‚´ìš©**: ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í•µì‹¬ ì •ë³´
2. ğŸ” **ìƒì„¸ ì„¤ëª…**: ë°°ê²½ê³¼ ë§¥ë½ì´ í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì„¤ëª…
3. ğŸ“Š **ë°ì´í„°/ìˆ˜ì¹˜**: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ê°€ ìˆë‹¤ë©´ í¬í•¨
4. ğŸ’¡ **ê²°ë¡ **: ìš”ì•½ê³¼ ì‹œì‚¬ì 

**ì¤‘ìš”ì‚¬í•­:**
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ì´ë¼ê³  ëª…ì‹œ
- ì§ˆë¬¸ ì–¸ì–´ì— ë§ì¶° ë‹µë³€ (í•œêµ­ì–´ ì§ˆë¬¸â†’í•œêµ­ì–´ ë‹µë³€)
- ê°„ê²°í•˜ë©´ì„œë„ ì •í™•í•œ ë‹µë³€ ì œê³µ

**ëŒ€í™” ë§¥ë½:**
{conversation_context}

**ì´ì „ ë‹µë³€(ìˆë‹¤ë©´):**
{previous_answer}

**ê²€ìƒ‰ëœ ì •ë³´:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€:**"""
        
        return ChatPromptTemplate.from_template(template)
    
    def _simple_route_query(self, question: str) -> SearchType:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ/ê·œì¹™ ê¸°ë°˜ ì¿¼ë¦¬ ë¼ìš°íŒ… (í˜„ì—… ì§€í–¥)
        - ìµœì‹ /ì‹¤ì‹œê°„/ê°€ê²©/ì¶œì‹œ ê´€ë ¨ â†’ WEB
        - ì—°ë„ê°€ 2025 ì´ìƒì´ê±°ë‚˜ 2024ê°€ ì•„ë‹Œ ëª…ì‹œì  ì—°ë„ â†’ WEB
        - ê·¸ ì™¸ AI/ì •ì±…/2024 ê´€ë ¨ â†’ VECTOR
        """
        q = question.lower()

        # 1) ì—°ë„ ê·œì¹™: 2025ë…„ ì´í›„, í˜¹ì€ 2024ê°€ ì•„ë‹Œ ëª…ì‹œì  ì—°ë„ë©´ ì›¹
        year_match = re.search(r"(20\d{2})", q)
        if year_match:
            year = int(year_match.group(1))
            if year >= 2025 or year != 2024:
                return SearchType.WEB

        # 2) ìµœì‹ /ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ â†’ ì›¹
        realtime_keywords = [
            'ì˜¤ëŠ˜', 'today', 'ì–´ì œ', 'yesterday', 'ìµœì‹ ', 'latest',
            'ì‹¤ì‹œê°„', 'realtime', 'ì§€ê¸ˆ', 'now', 'í˜„ì¬', 'current',
            'ë‰´ìŠ¤', 'news', 'ì†ë³´', 'breaking',
            'ì´ë²ˆì£¼', 'ì´ë²ˆ ì£¼', 'ì§€ë‚œì£¼', 'ì§€ë‚œ ì£¼', 'ì´ë²ˆë‹¬', 'ì´ë²ˆ ë‹¬', 'ì§€ë‚œë‹¬', 'ì§€ë‚œ ë‹¬',
            'ë¶„ê¸°', '1ë¶„ê¸°', '2ë¶„ê¸°', '3ë¶„ê¸°', '4ë¶„ê¸°',
            '1ì›”', '2ì›”', '3ì›”', '4ì›”', '5ì›”', '6ì›”', '7ì›”', '8ì›”', '9ì›”', '10ì›”', '11ì›”', '12ì›”',
            'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'
        ]
        if any(k in q for k in realtime_keywords):
            return SearchType.WEB

        # 3) AI/ì •ì±… ê´€ë ¨ (ë‚´ë¶€ ë¬¸ì„œ ì í•©)
        vector_keywords = [
            'ai', 'ì¸ê³µì§€ëŠ¥', 'ì •ì±…', 'policy', 'ì‚°ì—…', 'industry',
            'ê¸°ìˆ ', 'technology', 'ì „ëµ', 'strategy', 'íˆ¬ì', 'investment',
            '2024', 'í•œêµ­', 'korea', 'ë¯¸êµ­', 'usa', 'ì¤‘êµ­', 'china'
        ]
        if any(k in q for k in vector_keywords):
            return SearchType.VECTOR

        # ê¸°ë³¸ê°’: í•˜ì´ë¸Œë¦¬ë“œë¡œ ë‘ ì†ŒìŠ¤ ëª¨ë‘ ì‹œë„
        return SearchType.HYBRID

    def _bias_query_with_session_meta(self, question: str, session_id: str) -> str:
        """ì„¸ì…˜ ë©”íƒ€(last_entities)ë¥¼ í™œìš©í•´ ì¿¼ë¦¬ë¥¼ ì‚´ì§ ë³´ê°•"""
        try:
            meta = self.session_manager.get_session_meta(session_id)
            entities = meta.get('last_entities', {}) if isinstance(meta, dict) else {}
            keywords = entities.get('keywords', []) if isinstance(entities, dict) else []
            if keywords:
                # ì¤‘ë³µ ë°©ì§€
                to_add = [kw for kw in keywords if kw.lower() not in question.lower()]
                if to_add:
                    return question + " (ê´€ë ¨ í‚¤ì›Œë“œ: " + ", ".join(to_add[:5]) + ")"
        except Exception:
            pass
        return question

    def _expand_followup_query(self, question: str, session_id: str) -> str:
        """ì§§ê³  ëª¨í˜¸í•œ í›„ì† ì§ˆì˜ë¥¼ ì„¸ì…˜ ë©”íƒ€ì™€ ì§ì „ ë‹µë³€ìœ¼ë¡œ í™•ì¥
        ì˜ˆ: "í•œêµ­ê°€ê²©ì€?" â†’ "Galaxy S25 Ultra price in KRW, South Korea"
        """
        q = question.strip()
        # ê¸¸ì´ê°€ ì§§ê±°ë‚˜ ì§€ì‹œì–´ ìœ„ì£¼ì¼ ë•Œë§Œ í™•ì¥ ì‹œë„
        if len(q) > 20 and not any(k in q.lower() for k in ['krw', 'ì›', 'í•œêµ­']):
            return question

        meta = self.session_manager.get_session_meta(session_id)
        entities = meta.get('last_entities', {}) if isinstance(meta, dict) else {}
        keywords = entities.get('keywords', []) if isinstance(entities, dict) else []
        product_hint = ''
        if keywords:
            # ì œí’ˆ/ëª¨ë¸ ê´€ë ¨ í‚¤ì›Œë“œë§Œ ì¶”ë¦¼
            prios = ['s25 ultra', 's25', 'ultra', 'galaxy', 'ê°¤ëŸ­ì‹œ']
            ordered = [k for p in prios for k in keywords if p in k.lower()]
            product_hint = ordered[0] if ordered else keywords[0]
        if not product_hint:
            # ì§ì „ ë‹µë³€ì—ì„œ ê°„ë‹¨ ì¶”ì¶œ (ì˜ë¬¸ ëª¨ë¸ í¬í•¨ ì‹œ)
            recent = self.session_manager.get_messages(session_id, limit=4) or []
            prev_ans = next((m.content for m in reversed(recent) if m.role.value == 'assistant'), '')
            for cand in ['Galaxy S25 Ultra', 'Galaxy S25', 'S25 Ultra', 'S25']:
                if cand.lower() in prev_ans.lower():
                    product_hint = cand
                    break

        base = product_hint or ''
        # í†µí™”/ì§€ì—­ íŒíŠ¸
        suffix = ' price in KRW, South Korea'
        # ê¸°ì¡´ ì§ˆë¬¸ë„ í¬í•¨í•´ ì˜ë¯¸ ë³´ì¡´
        if base:
            return f"{base}{suffix} ({question})"
        else:
            return f"{question} (in KRW, South Korea)"

    def _llm_route_and_rewrite(self, question: str, session_id: str, default: SearchType) -> Tuple[SearchType, str]:
        """LLM ê¸°ë°˜ ë¼ìš°íŒ…/ì§ˆì˜ ì¬ì‘ì„± (ì¼ë°˜í™”ëœ ë°©ì‹)
        Returns: (search_type, expanded_query)
        """
        try:
            meta = self.session_manager.get_session_meta(session_id)
            entities = meta.get('last_entities', {}) if isinstance(meta, dict) else {}
            prev_msgs = self.session_manager.get_messages(session_id, limit=4) or []
            prev_answer = next((m.content for m in reversed(prev_msgs) if m.role.value == 'assistant'), "")

            prompt = ChatPromptTemplate.from_template(
                """
                You are a routing and query rewriting assistant.
                Given a user question, previous answer (optional), and lightweight entities (optional),
                decide the best search_type among: web | vector | hybrid.
                Then rewrite the user question into a fully self-contained query that disambiguates vague pronouns and context.

                Constraints:
                - Prefer vector for questions clearly answered by internal documents (policy/AI/2024/etc.)
                - Prefer web for real-time/consumer pricing/location/currency/availability and unknown years
                - Use hybrid if uncertain.
                - Output strict JSON only with keys: search_type, expanded_query

                User question: {question}
                Previous answer: {prev_answer}
                Entities (JSON): {entities}
                """
            )

            messages = prompt.format_messages(
                question=question,
                prev_answer=prev_answer,
                entities=json.dumps(entities, ensure_ascii=False)
            )
            raw = self.llm.invoke(messages).content
            data = json.loads(raw)
            st = str(data.get('search_type', default.value)).lower()
            if st not in ['web', 'vector', 'hybrid']:
                st = default.value
            expanded = data.get('expanded_query', question) or question
            return SearchType(st), expanded
        except Exception:
            # í´ë°±: ê¸°ì¡´ ë°”ì´ì–´ìŠ¤ + í™•ì¥
            biased = self._bias_query_with_session_meta(question, session_id)
            if len(question.strip()) <= 10:
                biased = self._expand_followup_query(biased, session_id)
            return default, biased
    
    def _get_cached_embedding(self, text: str) -> Optional[np.ndarray]:
        """ìºì‹œëœ ì„ë² ë”© ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
        embedding = self.cache.get_cached_embedding(text)
        if embedding is not None:
            return embedding
        
        try:
            # ìƒˆë¡œ ìƒì„±í•˜ê³  ìºì‹œ
            new_embedding = np.array(self.embeddings.embed_query(text))
            self.cache.cache_embedding(text, new_embedding)
            return new_embedding
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            return None
    
    def _vector_search(self, question: str) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.vector_store:
            logger.warning("Vector store not available")
            return []

        # ìºì‹œ í™•ì¸
        cached_results = self.cache.get_cached_search_results(question, "vector")
        if cached_results:
            logger.info("âœ… Using cached vector search results")
            return [SearchResult(**result) for result in cached_results]
        
        try:
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            documents = self.vector_store.similarity_search(
                question, 
                k=self.max_search_results
            )
            
            results = [SearchResult.from_document(doc) for doc in documents]
            
            # ê²°ê³¼ ìºì‹±
            results_dict = [result.to_dict() for result in results]
            self.cache.cache_search_results(question, results_dict, "vector")
            
            logger.info(f"âœ… Vector search found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def _web_search(self, question: str) -> List[SearchResult]:
        """ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.web_search_tool:
            logger.warning("Web search not available")
            return []
        
        # ìºì‹œ í™•ì¸
        cached_results = self.cache.get_cached_search_results(question, "web")
        if cached_results:
            logger.info("âœ… Using cached web search results")
            return [SearchResult(**result) for result in cached_results]
        
        try:
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            web_results = self.web_search_tool.invoke({"query": question})
            
            results = []
            for result in web_results[:self.max_search_results]:
                search_result = SearchResult(
                    content=result.get("content", ""),
                    source=result.get("url", ""),
                    score=0.8,  # ì›¹ ê²€ìƒ‰ì€ ê´€ë ¨ì„±ì´ ë†’ë‹¤ê³  ê°€ì •
                    title=result.get("title", ""),
                    metadata={"search_type": "web"}
                )
                results.append(search_result)
            
            # ê²°ê³¼ ìºì‹±
            results_dict = [result.to_dict() for result in results]
            self.cache.cache_search_results(question, results_dict, "web")
            
            logger.info(f"âœ… Web search found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"Web search failed: {e}")
            return []

    def _extract_entities_simple(self, text: str) -> Dict[str, Any]:
        """ê²½ëŸ‰ ì—”í‹°í‹° ì¶”ì¶œ(ê·œì¹™ ê¸°ë°˜): ì œí’ˆ/ëª¨ë¸/ê°€ê²©/í†µí™”"""
        entities: Dict[str, Any] = {}
        try:
            # ê°€ê²©/í†µí™”
            import re
            price_usd = re.findall(r"\$\s?([0-9][0-9,]*\.?[0-9]*)", text)
            price_krw = re.findall(r"([0-9][0-9,]*)\s?ì›", text)
            if price_usd:
                entities['price_usd'] = price_usd
            if price_krw:
                entities['price_krw'] = price_krw
            # ì œí’ˆ í‚¤ì›Œë“œ
            keywords = []
            for kw in ['galaxy', 'ê°¤ëŸ­ì‹œ', 'iphone', 'ì•„ì´í°', 'ultra', 'ìš¸íŠ¸ë¼', 'plus', 'í”ŒëŸ¬ìŠ¤', 's25', 's25 ultra', 's25 plus']:
                if kw.lower() in text.lower():
                    keywords.append(kw)
            if keywords:
                entities['keywords'] = list(set(keywords))
        except Exception:
            pass
        return entities
    
    def _parallel_search(self, question: str, search_types: List[SearchType]) -> List[SearchResult]:
        """ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰"""
        futures = []
        
        for search_type in search_types:
            if search_type == SearchType.VECTOR:
                future = self.executor.submit(self._vector_search, question)
                futures.append(future)
            elif search_type == SearchType.WEB:
                future = self.executor.submit(self._web_search, question)
                futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        all_results = []
        for future in as_completed(futures, timeout=30):  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
            try:
                results = future.result()
                all_results.extend(results)
            except Exception as e:
                logger.error(f"Search operation failed: {e}")
        
        # ì ìˆ˜ìˆœ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        unique_results = {}
        for result in all_results:
            key = hashlib.md5(result.content.encode()).hexdigest()[:16]
            if key not in unique_results or result.score > unique_results[key].score:
                unique_results[key] = result
        
        sorted_results = sorted(unique_results.values(), key=lambda x: x.score, reverse=True)
        return sorted_results[:self.max_search_results]
    
    def _format_context(self, results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, result in enumerate(results, 1):
            source = result.title or result.source
            content = result.content[:500]  # í† í° ì ˆì•½ì„ ìœ„í•´ ì œí•œ
            context_parts.append(f"[ë¬¸ì„œ {i}] {source}\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _generate_context_hash(self, context: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„± (ë‹µë³€ ìºì‹±ìš©)"""
        return hashlib.md5(context.encode()).hexdigest()[:16]
    
    def _is_contextual_operation(self, question: str) -> bool:
        """ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ì— ëŒ€í•œ ì‘ì—…(ë²ˆì—­/ìš”ì•½/ì •ë¦¬ ë“±)ì¸ì§€ ê°ì§€"""
        q = question.lower()
        patterns = [
            'ë²ˆì—­', 'translation', 'translate',
            'ìš”ì•½', 'summary', 'summarize', 'ì •ë¦¬',
            'ìœ„ ë‚´ìš©', 'ì´ ë‚´ìš©', 'ë°©ê¸ˆ', 'previous', 'above', 'the content', 'ê·¸ ë‚´ìš©', 'ê·¸ê±¸'
        ]
        return any(p in q for p in patterns)

    async def process_question(
        self, 
        question: str, 
        session_id: str,
        force_search_type: Optional[SearchType] = None,
        force_operation: Optional[str] = None
    ) -> RAGResponse:
        """
        ì§ˆë¬¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID
            force_search_type: ê°•ì œ ê²€ìƒ‰ íƒ€ì… (ì„ íƒì )
        
        Returns:
            RAGResponse ê°ì²´
        """
        start_time = time.time()
        
        try:
            # 1. ì„¸ì…˜ì— ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
            self.session_manager.add_message(
                session_id, MessageRole.USER, question
            )
            
            # 2. ì»¨í…ìŠ¤íŠ¸ì„± ì‘ì—…(ë²ˆì—­/ìš”ì•½/ìœ„ ë‚´ìš© ë“±) ì—¬ë¶€ íŒë‹¨
            # ê°€ë“œ1: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ operationì„ ì§€ì •í•˜ë©´ ê·œì¹™ë³´ë‹¤ ìš°ì„ 
            if force_operation:
                is_contextual = force_operation in [
                    'context', 'translate', 'summarize', 'rewrite'
                ]
            else:
                is_contextual = self._is_contextual_operation(question)
            
            # 3. ê²€ìƒ‰ íƒ€ì… ê²°ì • (ì»¨í…ìŠ¤íŠ¸ ì‘ì—…ì´ë©´ ê¸°ë³¸ê°’ VECTORë¡œ ì„¤ì •í•˜ì—¬ None ë°©ì§€)
            if is_contextual:
                search_type = SearchType.VECTOR
                expanded_query = question
            else:
                # 1ì°¨ ê·œì¹™ ë¼ìš°íŒ… í›„ LLM ê¸°ë°˜ ì¬í™•ì¸/ì¬ì‘ì„±ìœ¼ë¡œ ì¼ë°˜í™”
                initial = force_search_type or self._simple_route_query(question)
                search_type, expanded_query = self._llm_route_and_rewrite(question, session_id, initial)
            logger.info(f"ğŸ” Search type: {search_type}; expanded={expanded_query != question}")
            
            # 3. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
            # ìš”ì•½/ì •ë¦¬/ë²ˆì—­ ë“± ì»¨í…ìŠ¤íŠ¸ì„± ì‘ì—…ì€ ì´ˆê¸° ëŒ€í™”ê¹Œì§€ í¬í•¨í•˜ë„ë¡ ë” ê¸¸ê²Œ ê°€ì ¸ì™€ ëˆ„ë½ ë°©ì§€
            if is_contextual:
                # ë™ì  ë²„ì§“íŒ…: ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ ì—¬ìœ (ì˜ˆ: 2k í† í°) ë‚´ì—ì„œ
                # 1) ìµœì‹  ë©”ì‹œì§€, 2) ì•µì»¤(ëŒ€í™” ì‹œì‘/ì£¼ì œì „í™˜/ì•¡ì…˜ì•„ì´í…œ) ìš°ì„  í¬í•¨
                # ê°„ë‹¨ êµ¬í˜„: ë” ë§ì€ last_messages_limitë¥¼ ì‚¬ìš©í•˜ê³ , ì•µì»¤ëŠ” ë³„ë„ë¡œ ì•ìª½ì— ë¶™ì„
                anchor_msgs = []
                try:
                    full = self.session_manager.get_messages(session_id, include_system=True) or []
                    for m in full:
                        anchor = (m.metadata or {}).get('anchor') or (m.metadata or {}).get('topic_shift') or (m.metadata or {}).get('action_item')
                        if anchor:
                            anchor_msgs.append(m)
                except Exception:
                    pass

                conversation_context, conv_tokens = self.session_manager.get_conversation_context(
                    session_id, max_tokens=1800, last_messages_limit=60
                )
                # ì•µì»¤ë¥¼ ì•ìª½ì— prepend (ì¤‘ë³µì„ í”¼í•˜ê¸° ìœ„í•´ ê°„ë‹¨íˆ ë‚´ìš© ê¸°ì¤€)
                try:
                    anchor_texts = []
                    for am in anchor_msgs[:5]:
                        t = f"{am.role.value}: {am.content}"
                        if t not in conversation_context:
                            anchor_texts.append(t)
                    if anchor_texts:
                        conversation_context = "\n\n".join(anchor_texts) + "\n\n" + conversation_context
                except Exception:
                    pass
            else:
                conversation_context, conv_tokens = self.session_manager.get_conversation_context(
                    session_id, max_tokens=1000, last_messages_limit=12
                )
            
            # 4. ê²€ìƒ‰ ìˆ˜í–‰ (ì»¨í…ìŠ¤íŠ¸ ì‘ì—…ì´ë©´ ê²€ìƒ‰ ìƒëµí•˜ê³  ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©)
            if is_contextual:
                # ê°€ë“œ3: ìµœê·¼ assistant ë©”ì‹œì§€ê°€ ì—†ë‹¤ë©´ 'ëª…ì‹œì  ì•ˆë‚´ + ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì²˜ë¦¬'ë¡œ í´ë°±í•´ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
                recent_msgs = self.session_manager.get_messages(session_id, limit=4) or []
                has_recent_assistant = any(m.role.value == 'assistant' for m in recent_msgs)

                if not has_recent_assistant and not force_operation:
                    logger.info("â„¹ï¸ No recent assistant message; proceeding without retrieval and with user guidance")
                    search_results = []
                    # ì§ˆë¬¸ ì•ì— ì•ˆë‚´ë¥¼ ë§ë¶™ì—¬ LLMì´ ìƒí™©ì„ ëª…í™•íˆ ì•Œë„ë¡ í•¨
                    question = (
                        "ì´ì „ ëŒ€í™” ë§¥ë½ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ í˜„ì¬ ë¬¸ì¥ë§Œì„ ëŒ€ìƒìœ¼ë¡œ ì‘ì—…í•˜ì„¸ìš”.\n\n" + question
                    )
                else:
                    search_results = []
            else:
                # LLM ì¬ì‘ì„± ê²°ê³¼ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ì„¸ì…˜ ì—”í‹°í‹° ë³´ê°•/ì§§ì€ í›„ì† í™•ì¥
                q_use = expanded_query or question
                if q_use == question:
                    q_use = self._bias_query_with_session_meta(q_use, session_id)
                    if len(question.strip()) <= 10:
                        q_use = self._expand_followup_query(q_use, session_id)
                if search_type == SearchType.HYBRID:
                    search_results = self._parallel_search(q_use, [SearchType.VECTOR, SearchType.WEB])
                elif search_type == SearchType.VECTOR:
                    search_results = self._vector_search(q_use)
                    # ë²¡í„° ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                    if not search_results and self.web_search_tool:
                        logger.info("â„¹ï¸ No vector results, falling back to web search")
                        search_results = self._web_search(q_use)
                        search_type = SearchType.WEB
                elif search_type == SearchType.WEB:
                    search_results = self._web_search(q_use)
                else:
                    search_results = []
            
            # 5. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            # ì»¨í…ìŠ¤íŠ¸ì„± ì‘ì—…ì¼ ë•ŒëŠ” ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¹„ì›Œ LLMì´ ëŒ€í™” ë§¥ë½ì—ë§Œ ì§‘ì¤‘í•˜ë„ë¡ í•¨
            context = self._format_context(search_results) if not is_contextual else ""
            context_hash = self._generate_context_hash(context)
            
            # 6. ìºì‹œëœ ë‹µë³€ í™•ì¸
            cached_answer = self.cache.get_cached_answer(question, context_hash)
            if cached_answer:
                logger.info("âœ… Using cached answer")
                
                # ì„¸ì…˜ì— ìºì‹œëœ ë‹µë³€ ì¶”ê°€
                self.session_manager.add_message(
                    session_id, MessageRole.ASSISTANT, cached_answer
                )
                
                response_time = time.time() - start_time
                return RAGResponse(
                    answer=cached_answer,
                    sources=search_results,
                    search_type=search_type,
                    response_time=response_time,
                    cached=True
                )
            
            # ì´ì „ assistant ë‹µë³€(ìˆë‹¤ë©´) ì¶”ì¶œí•´ í›„ì† ì§ˆë¬¸ í’ˆì§ˆ ê°œì„ 
            recent_msgs_for_prev = self.session_manager.get_messages(session_id, limit=4) or []
            prev_assistant = next((m.content for m in reversed(recent_msgs_for_prev) if m.role.value == 'assistant'), "")

            # 7. ìƒˆ ë‹µë³€ ìƒì„± (íƒ€ì„ì•„ì›ƒ ê°€ë“œë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€)
            try:
                answer = await asyncio.wait_for(
                    self._generate_answer_async(
                        question, context, conversation_context, prev_assistant
                    ), timeout=30
                )
            except asyncio.TimeoutError:
                logger.error("LLM generation timeout; returning fallback message")
                answer = "ìš”ì²­ì´ ì˜ˆìƒë³´ë‹¤ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            
            # 8. ë‹µë³€ ìºì‹± ë° ì„¸ì…˜ ë©”íƒ€ ì—…ë°ì´íŠ¸
            self.cache.cache_answer(question, answer, context_hash)
            try:
                # ê°„ë‹¨ ì—”í‹°í‹° ì¶”ì¶œ ë° ì†ŒìŠ¤ ë³´ì¡´
                entities = self._extract_entities_simple(answer)
                sources_compact = [
                    {
                        'title': s.title,
                        'source': s.source,
                        'score': s.score
                    } for s in (search_results[:3] if search_results else [])
                ]
                self.session_manager.set_session_meta(session_id, {
                    'last_answer': answer,
                    'last_entities': entities,
                    'last_sources': sources_compact
                })
            except Exception as _:
                pass
            
            # 9. ì„¸ì…˜ì— ë‹µë³€ ì¶”ê°€
            self.session_manager.add_message(
                session_id, MessageRole.ASSISTANT, answer
            )
            
            response_time = time.time() - start_time
            
            logger.info(f"âœ… RAG completed in {response_time:.2f}s")
            
            return RAGResponse(
                answer=answer,
                sources=search_results,
                search_type=search_type,
                response_time=response_time,
                cached=False,
                tokens_used=len(answer.split()) + conv_tokens  # ëŒ€ëµì ì¸ í† í° ìˆ˜
            )
            
        except Exception as e:
            logger.error(f"RAG processing failed: {e}")
            error_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            response_time = time.time() - start_time
            return RAGResponse(
                answer=error_answer,
                sources=[],
                search_type=search_type,
                response_time=response_time,
                cached=False
            )
    
    async def _generate_answer_async(
        self, 
        question: str, 
        context: str, 
        conversation_context: str,
        previous_answer: str
    ) -> str:
        """ë¹„ë™ê¸° ë‹µë³€ ìƒì„±"""
        try:
            loop = asyncio.get_event_loop()
            
            # LLM í˜¸ì¶œì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            answer = await loop.run_in_executor(
                self.executor,
                self._generate_answer_sync,
                question,
                context,
                conversation_context,
                previous_answer
            )
            
            return answer
            
        except Exception as e:
            logger.error(f"Async answer generation failed: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_answer_sync(
        self, 
        question: str, 
        context: str, 
        conversation_context: str,
        previous_answer: str
    ) -> str:
        """ë™ê¸° ë‹µë³€ ìƒì„±"""
        try:
            answer = self.rag_chain.invoke({
                "question": question,
                "context": context,
                "conversation_context": conversation_context,
                "previous_answer": previous_answer
            })
            return answer
            
        except Exception as e:
            logger.error(f"Answer generation failed: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def get_engine_stats(self) -> Dict[str, Any]:
        """ì—”ì§„ í†µê³„ ì¡°íšŒ"""
        cache_health = self.cache.get_health()
        session_stats = self.session_manager.get_session_stats()
        
        return {
            'model_name': self.model_name,
            'cache_health': cache_health,
            'session_stats': session_stats,
            'vector_store_available': self.vector_store is not None,
            'web_search_available': self.web_search_tool is not None
        }
