"""
Optimized RAG Engine
í˜„ì—… íŒ¨í„´ì„ ì ìš©í•œ ê³ ì„±ëŠ¥ RAG ì‹œìŠ¤í…œ

Key Improvements:
- ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ê³„ì‚° ì œê±°
- ë³‘ë ¬ ê²€ìƒ‰ ì²˜ë¦¬
- ê°„ì†Œí™”ëœ íŒŒì´í”„ë¼ì¸
- Circuit breaker íŒ¨í„´
- í† í° ìµœì í™”
"""

import asyncio
import re
import hashlib
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from .cache_manager import RAGCacheManager, get_cache_manager
from .session_manager import SessionManager, MessageRole, get_session_manager

logger = logging.getLogger(__name__)


class SearchType(str, Enum):
    VECTOR = "vector"
    WEB = "web"
    HYBRID = "hybrid"


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""
    content: str
    source: str
    score: float
    title: str = ""
    metadata: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'content': self.content,
            'source': self.source, 
            'score': self.score,
            'title': self.title,
            'metadata': self.metadata or {}
        }
    
    @classmethod
    def from_document(cls, doc: Document) -> 'SearchResult':
        """Langchain Documentì—ì„œ SearchResult ìƒì„±"""
        metadata = doc.metadata or {}
        return cls(
            content=doc.page_content,
            source=metadata.get('source', 'unknown'),
            score=metadata.get('score', 0.0),
            title=metadata.get('title', ''),
            metadata=metadata
        )


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ëª¨ë¸"""
    answer: str
    sources: List[SearchResult]
    search_type: SearchType
    response_time: float
    cached: bool = False
    tokens_used: int = 0


class OptimizedRAGEngine:
    """
    ìµœì í™”ëœ RAG ì—”ì§„
    
    Features:
    - Multi-level caching (embedding, search, answer)
    - Parallel search execution
    - Smart routing (simple keyword-based)
    - Circuit breaker for external services
    - Token optimization
    """
    
    def __init__(
        self,
        vector_store=None,
        model_name: str = "gpt-3.5-turbo",
        temperature: float = 0,
        max_search_results: int = 8,
        max_context_tokens: int = 3000
    ):
        self.vector_store = vector_store
        self.model_name = model_name
        self.temperature = temperature
        self.max_search_results = max_search_results
        self.max_context_tokens = max_context_tokens
        
        # Core components
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        self.cache = get_cache_manager()
        self.session_manager = get_session_manager()
        
        # Thread pool for parallel operations
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # Web search (optional)
        self.web_search_tool = None
        try:
            from langchain_community.tools.tavily_search import TavilySearchResults
            self.web_search_tool = TavilySearchResults(
                max_results=3,
                search_depth="advanced",
                include_answer=True,
                include_raw_content=True,
                include_images=False
            )
            logger.info("âœ… Web search tool initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ Web search not available: {e}")
        
        # Optimized prompt
        self.rag_prompt = self._create_optimized_prompt()
        self.rag_chain = self.rag_prompt | self.llm | StrOutputParser()
        
        logger.info(f"OptimizedRAGEngine initialized with model: {model_name}")
    
    def _create_optimized_prompt(self) -> ChatPromptTemplate:
        """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        template = """ë‹¹ì‹ ì€ ì „ë¬¸ AI ì—°êµ¬ ë¶„ì„ê°€ìž…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
1. ðŸ“‹ **í•µì‹¬ ë‚´ìš©**: ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í•µì‹¬ ì •ë³´
2. ðŸ” **ìƒì„¸ ì„¤ëª…**: ë°°ê²½ê³¼ ë§¥ë½ì´ í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì„¤ëª…
3. ðŸ“Š **ë°ì´í„°/ìˆ˜ì¹˜**: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ê°€ ìžˆë‹¤ë©´ í¬í•¨
4. ðŸ’¡ **ê²°ë¡ **: ìš”ì•½ê³¼ ì‹œì‚¬ì 

**ì¤‘ìš”ì‚¬í•­:**
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ì´ë¼ê³  ëª…ì‹œ
- ì§ˆë¬¸ ì–¸ì–´ì— ë§žì¶° ë‹µë³€ (í•œêµ­ì–´ ì§ˆë¬¸â†’í•œêµ­ì–´ ë‹µë³€)
- ê°„ê²°í•˜ë©´ì„œë„ ì •í™•í•œ ë‹µë³€ ì œê³µ

**ëŒ€í™” ë§¥ë½:**
{conversation_context}

**ê²€ìƒ‰ëœ ì •ë³´:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€:**"""
        
        return ChatPromptTemplate.from_template(template)
    
    def _simple_route_query(self, question: str) -> SearchType:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ/ê·œì¹™ ê¸°ë°˜ ì¿¼ë¦¬ ë¼ìš°íŒ… (í˜„ì—… ì§€í–¥)
        - ìµœì‹ /ì‹¤ì‹œê°„/ê°€ê²©/ì¶œì‹œ ê´€ë ¨ â†’ WEB
        - ì—°ë„ê°€ 2025 ì´ìƒì´ê±°ë‚˜ 2024ê°€ ì•„ë‹Œ ëª…ì‹œì  ì—°ë„ â†’ WEB
        - ê·¸ ì™¸ AI/ì •ì±…/2024 ê´€ë ¨ â†’ VECTOR
        """
        q = question.lower()

        # 1) ì—°ë„ ê·œì¹™: 2025ë…„ ì´í›„, í˜¹ì€ 2024ê°€ ì•„ë‹Œ ëª…ì‹œì  ì—°ë„ë©´ ì›¹
        year_match = re.search(r"(20\d{2})", q)
        if year_match:
            year = int(year_match.group(1))
            if year >= 2025 or year != 2024:
                return SearchType.WEB

        # 2) ìµœì‹ /ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ â†’ ì›¹
        realtime_keywords = [
            'ì˜¤ëŠ˜', 'today', 'ì–´ì œ', 'yesterday', 'ìµœì‹ ', 'latest',
            'ì‹¤ì‹œê°„', 'realtime', 'ì§€ê¸ˆ', 'now', 'í˜„ìž¬', 'current',
            'ë‰´ìŠ¤', 'news', 'ì†ë³´', 'breaking',
            'ì´ë²ˆì£¼', 'ì´ë²ˆ ì£¼', 'ì§€ë‚œì£¼', 'ì§€ë‚œ ì£¼', 'ì´ë²ˆë‹¬', 'ì´ë²ˆ ë‹¬', 'ì§€ë‚œë‹¬', 'ì§€ë‚œ ë‹¬',
            'ë¶„ê¸°', '1ë¶„ê¸°', '2ë¶„ê¸°', '3ë¶„ê¸°', '4ë¶„ê¸°',
            '1ì›”', '2ì›”', '3ì›”', '4ì›”', '5ì›”', '6ì›”', '7ì›”', '8ì›”', '9ì›”', '10ì›”', '11ì›”', '12ì›”',
            'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'
        ]
        if any(k in q for k in realtime_keywords):
            return SearchType.WEB

        # 3) ì†Œë¹„ìž ì œí’ˆ/ê°€ê²©/ì¶œì‹œ ê´€ë ¨ í‚¤ì›Œë“œ â†’ ì›¹
        product_price_keywords = [
            'ì•„ì´í°', 'iphone', 'ê°¤ëŸ­ì‹œ', 'galaxy', 'ë§¥ë¶', 'macbook', 'ì• í”Œ', 'apple', 'ì‚¼ì„±', 'samsung',
            'ê°€ê²©', 'price', 'ì¶œì‹œ', 'ë°œí‘œ', 'ë°œë§¤', 'ëŸ°ì¹­', 'official', 'event', 'ì–¸ì œ', 'ì–¼ë§ˆ'
        ]
        if any(k in q for k in product_price_keywords):
            return SearchType.WEB

        # 4) AI/ì •ì±… ê´€ë ¨ (ë‚´ë¶€ ë¬¸ì„œ ì í•©)
        vector_keywords = [
            'ai', 'ì¸ê³µì§€ëŠ¥', 'ì •ì±…', 'policy', 'ì‚°ì—…', 'industry',
            'ê¸°ìˆ ', 'technology', 'ì „ëžµ', 'strategy', 'íˆ¬ìž', 'investment',
            '2024', 'í•œêµ­', 'korea', 'ë¯¸êµ­', 'usa', 'ì¤‘êµ­', 'china'
        ]
        if any(k in q for k in vector_keywords):
            return SearchType.VECTOR

        # ê¸°ë³¸ê°’: í•˜ì´ë¸Œë¦¬ë“œë¡œ ë‘ ì†ŒìŠ¤ ëª¨ë‘ ì‹œë„
        return SearchType.HYBRID
    
    def _get_cached_embedding(self, text: str) -> Optional[np.ndarray]:
        """ìºì‹œëœ ìž„ë² ë”© ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
        embedding = self.cache.get_cached_embedding(text)
        if embedding is not None:
            return embedding
        
        try:
            # ìƒˆë¡œ ìƒì„±í•˜ê³  ìºì‹œ
            new_embedding = np.array(self.embeddings.embed_query(text))
            self.cache.cache_embedding(text, new_embedding)
            return new_embedding
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            return None
    
    def _vector_search(self, question: str) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.vector_store:
            logger.warning("Vector store not available")
            return []
        
        # ìºì‹œ í™•ì¸
        cached_results = self.cache.get_cached_search_results(question, "vector")
        if cached_results:
            logger.info("âœ… Using cached vector search results")
            return [SearchResult(**result) for result in cached_results]
        
        try:
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            documents = self.vector_store.similarity_search(
                question, 
                k=self.max_search_results
            )
            
            results = [SearchResult.from_document(doc) for doc in documents]
            
            # ê²°ê³¼ ìºì‹±
            results_dict = [result.to_dict() for result in results]
            self.cache.cache_search_results(question, results_dict, "vector")
            
            logger.info(f"âœ… Vector search found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def _web_search(self, question: str) -> List[SearchResult]:
        """ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.web_search_tool:
            logger.warning("Web search not available")
            return []
        
        # ìºì‹œ í™•ì¸
        cached_results = self.cache.get_cached_search_results(question, "web")
        if cached_results:
            logger.info("âœ… Using cached web search results")
            return [SearchResult(**result) for result in cached_results]
        
        try:
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            web_results = self.web_search_tool.invoke({"query": question})
            
            results = []
            for result in web_results[:self.max_search_results]:
                search_result = SearchResult(
                    content=result.get("content", ""),
                    source=result.get("url", ""),
                    score=0.8,  # ì›¹ ê²€ìƒ‰ì€ ê´€ë ¨ì„±ì´ ë†’ë‹¤ê³  ê°€ì •
                    title=result.get("title", ""),
                    metadata={"search_type": "web"}
                )
                results.append(search_result)
            
            # ê²°ê³¼ ìºì‹±
            results_dict = [result.to_dict() for result in results]
            self.cache.cache_search_results(question, results_dict, "web")
            
            logger.info(f"âœ… Web search found {len(results)} results")
            return results
            
        except Exception as e:
            logger.error(f"Web search failed: {e}")
            return []
    
    def _parallel_search(self, question: str, search_types: List[SearchType]) -> List[SearchResult]:
        """ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰"""
        futures = []
        
        for search_type in search_types:
            if search_type == SearchType.VECTOR:
                future = self.executor.submit(self._vector_search, question)
                futures.append(future)
            elif search_type == SearchType.WEB:
                future = self.executor.submit(self._web_search, question)
                futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        all_results = []
        for future in as_completed(futures, timeout=30):  # 30ì´ˆ íƒ€ìž„ì•„ì›ƒ
            try:
                results = future.result()
                all_results.extend(results)
            except Exception as e:
                logger.error(f"Search operation failed: {e}")
        
        # ì ìˆ˜ìˆœ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        unique_results = {}
        for result in all_results:
            key = hashlib.md5(result.content.encode()).hexdigest()[:16]
            if key not in unique_results or result.score > unique_results[key].score:
                unique_results[key] = result
        
        sorted_results = sorted(unique_results.values(), key=lambda x: x.score, reverse=True)
        return sorted_results[:self.max_search_results]
    
    def _format_context(self, results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, result in enumerate(results, 1):
            source = result.title or result.source
            content = result.content[:500]  # í† í° ì ˆì•½ì„ ìœ„í•´ ì œí•œ
            context_parts.append(f"[ë¬¸ì„œ {i}] {source}\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _generate_context_hash(self, context: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„± (ë‹µë³€ ìºì‹±ìš©)"""
        return hashlib.md5(context.encode()).hexdigest()[:16]
    
    def _is_contextual_operation(self, question: str) -> bool:
        """ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ì— ëŒ€í•œ ìž‘ì—…(ë²ˆì—­/ìš”ì•½/ì •ë¦¬ ë“±)ì¸ì§€ ê°ì§€"""
        q = question.lower()
        patterns = [
            'ë²ˆì—­', 'translation', 'translate',
            'ìš”ì•½', 'summary', 'summarize', 'ì •ë¦¬',
            'ìœ„ ë‚´ìš©', 'ì´ ë‚´ìš©', 'ë°©ê¸ˆ', 'previous', 'above', 'the content', 'ê·¸ ë‚´ìš©', 'ê·¸ê±¸'
        ]
        return any(p in q for p in patterns)

    async def process_question(
        self, 
        question: str, 
        session_id: str,
        force_search_type: Optional[SearchType] = None,
        force_operation: Optional[str] = None
    ) -> RAGResponse:
        """
        ì§ˆë¬¸ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            question: ì‚¬ìš©ìž ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID
            force_search_type: ê°•ì œ ê²€ìƒ‰ íƒ€ìž… (ì„ íƒì )
        
        Returns:
            RAGResponse ê°ì²´
        """
        start_time = time.time()
        
        try:
            # 1. ì„¸ì…˜ì— ì‚¬ìš©ìž ì§ˆë¬¸ ì¶”ê°€
            self.session_manager.add_message(
                session_id, MessageRole.USER, question
            )
            
            # 2. ì»¨í…ìŠ¤íŠ¸ì„± ìž‘ì—…(ë²ˆì—­/ìš”ì•½/ìœ„ ë‚´ìš© ë“±) ì—¬ë¶€ íŒë‹¨
            # ê°€ë“œ1: ì‚¬ìš©ìžê°€ ëª…ì‹œì ìœ¼ë¡œ operationì„ ì§€ì •í•˜ë©´ ê·œì¹™ë³´ë‹¤ ìš°ì„ 
            if force_operation:
                is_contextual = force_operation in [
                    'context', 'translate', 'summarize', 'rewrite'
                ]
            else:
                is_contextual = self._is_contextual_operation(question)
            
            # 3. ê²€ìƒ‰ íƒ€ìž… ê²°ì • (ì»¨í…ìŠ¤íŠ¸ ìž‘ì—…ì´ë©´ ê²€ìƒ‰ ìƒëžµ)
            search_type = None
            if not is_contextual:
                search_type = force_search_type or self._simple_route_query(question)
                logger.info(f"ðŸ” Search type: {search_type}")
            
            # 3. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ (ìµœê·¼ì„± ì œí•œ í¬í•¨)
            conversation_context, conv_tokens = self.session_manager.get_conversation_context(
                session_id, max_tokens=1000
            )
            
            # 4. ê²€ìƒ‰ ìˆ˜í–‰ (ì»¨í…ìŠ¤íŠ¸ ìž‘ì—…ì´ë©´ ê²€ìƒ‰ ìƒëžµí•˜ê³  ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©)
            if is_contextual:
                # ê°€ë“œ3: ìµœê·¼ assistant ë©”ì‹œì§€ê°€ ì—†ë‹¤ë©´ ê²€ìƒ‰ ê¸°ë°˜ìœ¼ë¡œ í´ë°±
                recent_msgs = self.session_manager.get_messages(session_id, limit=4) or []
                has_recent_assistant = any(m.role.value == 'assistant' for m in recent_msgs)

                if not has_recent_assistant and not force_operation:
                    logger.info("â„¹ï¸ No recent assistant message; falling back to retrieval for contextual request")
                    is_contextual = False
                else:
                    search_results = []
            else:
                if search_type == SearchType.HYBRID:
                    search_results = self._parallel_search(question, [SearchType.VECTOR, SearchType.WEB])
                elif search_type == SearchType.VECTOR:
                    search_results = self._vector_search(question)
                    # ë²¡í„° ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìžë™ìœ¼ë¡œ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                    if not search_results and self.web_search_tool:
                        logger.info("â„¹ï¸ No vector results, falling back to web search")
                        search_results = self._web_search(question)
                        search_type = SearchType.WEB
                elif search_type == SearchType.WEB:
                    search_results = self._web_search(question)
                else:
                    search_results = []
            
            # 5. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            # ì»¨í…ìŠ¤íŠ¸ì„± ìž‘ì—…ì¼ ë•ŒëŠ” ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¹„ì›Œ LLMì´ ëŒ€í™” ë§¥ë½ì—ë§Œ ì§‘ì¤‘í•˜ë„ë¡ í•¨
            context = self._format_context(search_results) if not is_contextual else ""
            context_hash = self._generate_context_hash(context)
            
            # 6. ìºì‹œëœ ë‹µë³€ í™•ì¸
            cached_answer = self.cache.get_cached_answer(question, context_hash)
            if cached_answer:
                logger.info("âœ… Using cached answer")
                
                # ì„¸ì…˜ì— ìºì‹œëœ ë‹µë³€ ì¶”ê°€
                self.session_manager.add_message(
                    session_id, MessageRole.ASSISTANT, cached_answer
                )
                
                response_time = time.time() - start_time
                return RAGResponse(
                    answer=cached_answer,
                    sources=search_results,
                    search_type=search_type,
                    response_time=response_time,
                    cached=True
                )
            
            # 7. ìƒˆ ë‹µë³€ ìƒì„±
            answer = await self._generate_answer_async(
                question, context, conversation_context
            )
            
            # 8. ë‹µë³€ ìºì‹±
            self.cache.cache_answer(question, answer, context_hash)
            
            # 9. ì„¸ì…˜ì— ë‹µë³€ ì¶”ê°€
            self.session_manager.add_message(
                session_id, MessageRole.ASSISTANT, answer
            )
            
            response_time = time.time() - start_time
            
            logger.info(f"âœ… RAG completed in {response_time:.2f}s")
            
            return RAGResponse(
                answer=answer,
                sources=search_results,
                search_type=search_type,
                response_time=response_time,
                cached=False,
                tokens_used=len(answer.split()) + conv_tokens  # ëŒ€ëžµì ì¸ í† í° ìˆ˜
            )
            
        except Exception as e:
            logger.error(f"RAG processing failed: {e}")
            error_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            response_time = time.time() - start_time
            return RAGResponse(
                answer=error_answer,
                sources=[],
                search_type=search_type,
                response_time=response_time,
                cached=False
            )
    
    async def _generate_answer_async(
        self, 
        question: str, 
        context: str, 
        conversation_context: str
    ) -> str:
        """ë¹„ë™ê¸° ë‹µë³€ ìƒì„±"""
        try:
            loop = asyncio.get_event_loop()
            
            # LLM í˜¸ì¶œì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            answer = await loop.run_in_executor(
                self.executor,
                self._generate_answer_sync,
                question,
                context,
                conversation_context
            )
            
            return answer
            
        except Exception as e:
            logger.error(f"Async answer generation failed: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_answer_sync(
        self, 
        question: str, 
        context: str, 
        conversation_context: str
    ) -> str:
        """ë™ê¸° ë‹µë³€ ìƒì„±"""
        try:
            answer = self.rag_chain.invoke({
                "question": question,
                "context": context,
                "conversation_context": conversation_context
            })
            return answer
            
        except Exception as e:
            logger.error(f"Answer generation failed: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def get_engine_stats(self) -> Dict[str, Any]:
        """ì—”ì§„ í†µê³„ ì¡°íšŒ"""
        cache_health = self.cache.get_health()
        session_stats = self.session_manager.get_session_stats()
        
        return {
            'model_name': self.model_name,
            'cache_health': cache_health,
            'session_stats': session_stats,
            'vector_store_available': self.vector_store is not None,
            'web_search_available': self.web_search_tool is not None
        }
